{"componentChunkName":"component---src-pages-content-integration-onprem-online-md","path":"/content/integration/onprem-online/","result":{"pageContext":{"frontmatter":{"title":"onprem-online","weight":400},"relativePagePath":"/content/integration/onprem-online.md","titleType":"page","MdxNode":{"id":"7f9d0f03-2449-5851-b960-07be66eb9608","children":[],"parent":"dea47313-5490-564a-9467-0bc61dc621fc","internal":{"content":"---\ntitle: onprem-online\nweight: 400\n---\n\n- [Introduction](#introduction)\n- [Prepare For Installation](#prepare-for-installation)\n- [Run the Integration Cloud Pak install](#run-the-integration-cloud-pak-install)\n- [Deploy Capabilities](#deploy-capabilities)\n- [Example files](#example-files)\n  - [config.yaml](#configyaml)\n\n## Introduction\n\nThis page describes all the steps on how to deploy the Integration Cloud Pak to a VMWARE onprem environment using the IBM Entitled registry. The steps below includes instructions to:\n\n1. Prepare the bastion/installation node for the installation\n2. Run the Integration Cloud Pak installer to deploy to an existing OpenShift cluster\n\n## Prepare For Installation\n\nIn many production scenarios the master nodes may not be accessed via SSH, in which case we have to create or choose a bastion node in order to proceed with the installation. This can be a worker node within the cluster or a VM set up specifically for this task.\n\n**Installer Node requirements:**\n\n- Sufficient resources of `4cpu 16GB ram ~120 GB Diskspace`\n- OpenShift CLI, which can be installed following the instructions [here on IBM Cloud](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli).\n- Docker (v2.2 is compliant and works with Openshift)\n- Kubernetes CLI kubectl\n\nOnce the CLIs are installed, check if you can login to your openshift environment:\n\n  1. Get login token from openshift console\n  2. Run the `oc login` command from a terminal shell.\n  3. You should see the cluster logged in message along with list of projects.\n  \n**Installing on Master or Infra node:**  \nThe value of the master, proxy, and management parameters is an array and can have multiple nodes. Due to a limitation from OpenShift, if you want to deploy IBM Cloud Private on any OpenShift master or infrastructure node, you must label the node as an OpenShift compute node with the following command:\n\n```bash\noc label node &lt;master node host name/infrastructure node host name> node-role.kubernetes.io/compute=true\n```\n\n## Run the Integration Cloud Pak install\n\nIBM Cloud Pak for Integration provides a single installer that installs ICP as well loads all the helm charts for integration capabilities. In this example CP4I will be installed on the master node.\n\n1. Download the Cloud Pak and extract the contents onto the bastion node. See [Pre-requisites](../pre-reqs) for guidance. It is a general recommendation to create a directory in /opt and extract into that directory:\n\n    ```bash\n    tar xf ibm-cp-int-2019.4.1-online.tar.gz --directory /opt/cp4i\n    ```\n\n    Once untarred, you can navigate to the directory where the packages was untarred to and type `tree`.  It will look like the below\n    ![Output of tree command](1.untar-cp4i.png)\n\n2. Load the images onto your local docker registry:\n\n    ```bash\n    sudo docker load -i installer_files/cluster/images/icp-inception-3.2.2.tgz\n    ```\n\n3. Change to the `installer_files/cluster/` directory. Place the cluster configuration files (kubeconfig) in the `installer_files/cluster/` directory. You can also use the following command after using oc login as admin.  Make sure your file only has one cluster context defined with in it, and that context is the location of your target cluster.\n\n    ```bash\n    oc config view > kubeconfig\n    ```\n\n4. Note down the IP addresses of OpenShift worker nodes. To get the IP addresses of the worker nodes, run:\n\n    ```bash\n    oc get nodes -o wide\n    ```\n\n5. Navigate to your cluster directory `/opt/cp4i/installer/cluster`.\n6. Edit the config.yaml with the information you have collected above. See the example at the [end of the page](#configyaml) for guidance.  \nHere are the fields to update with your respective values based on your environment:\n\n    - under cluster_nodes heading -> set the hostnames for `Master`, `Proxy` and `Management`.  For non-prdoduction systems, setting and proxy to the same host is fine. Use the short name for your nodes (e.g. compute1, compute2 etc)\n    - under storage_class -> choose your default storage class here - use `oc get sc` to get a list of available storageclasses.\n    - docker_user -> `ekey`\n    - docker_password -> set this to your entitlement key\n\n    Instructions to get your entitlement key can be found [here](https://github.ibm.com/CloudPakOpenContent/cloudpak-entitlement) if you are an IBM employee.\n\n7. Run the installer with:\n\n    ```bash\n    sudo docker run -t --net=host -e LICENSE=accept -v $(pwd):/installer/cluster:z -v /var/run:/var/run:z -v /etc/docker:/etc/docker:z --security-opt label:disable ibmcom/icp-inception-amd64:3.2.2 addon -vvv | tee install.log\n    ```\n\n8. If the namespaces for the different capabilities did not create you can create them manually using the scripts in `installer_files/cluster/resources` e.g. ace.yaml, apic.yaml.  Simply run each script using this syntax:\n\n    ```bash\n    oc create -f &lt;scriptname>.yaml\n    ```\n\n9. Once the process is complete, you will need to create your `ibm_entitlement_key` secrets in all of the main component namespaces.  You can accomplish this by running the `create_secrets.sh` script. Before doing so, export these two variables:\n\n    ```bash\n    export DOCKER_REGISTRY_USER=ekey\n    export DOCKER_REGISTRY_PASS=&lt;your entitlement key>\n    ```\n\n10. It will be helpful to understand what your proxy node address is, as it will be referenced several time when deploying the individual capabilities.  Run this command and take note of its output:\n\n    ```bash\n    oc get configmap -n kube-public ibmcloud-cluster-info -o=jsonpath=\"{.data.proxy_address}\"`\n    ```\n\n## Deploy Capabilities\n\nIt is recommended that you install the Tracing capability first\n\n- [Tracing](../deploy-tracing)\n- [App Connect](../deploy-integration)\n- [API Connect](../deploy-api-mgmt)\n- [MQ](../deploy-queue-manager)\n- [Event Streams](../deploy-eventstreams)\n- [Aspera](../deploy-fast-file-transfer)\n- [DataPower](../deploy-secure-gateway)\n- [Asset Repository](../deploy-asset-repo)\n\n## Example files\n\nThis section contains examples of files you will be using throughout the installation. Refer to them for guidance on how to populate your own version of the files.\n\n### config.yaml\n\n```yaml\n# Nodes selected to run common services components.\n#\n# The value of the master, proxy, and management parameters is an array,\n# by providing multiple nodes the common services will be configured in\n# a high availability configuration.\n#\n# It is recommended to install the components onto one or more openshift\n# worker nodes. The master, proxy, and management components can all share\n# the same node or set of nodes.\ncluster_nodes:\n  master:\n    - compute1\n  proxy:\n    - compute1\n  management:\n    - compute2\n\n# This storage class is used to store persistent data for the common services\n# components\nstorage_class: csi-cephfs\n\n## You can set a different storage class for storing log data.\n## By default it will use the value of storage_class.\n# elasticsearch_storage_class:\n\n# These settings enable the installer to install common services from the IBM\n# Entitled Registry. You will need to supply your entitlement key as described\n# at https://github.com/ibm/charts\nprivate_registry_enabled: true\nimage_repo: cp.icr.io/cp/icp-foundation\ndocker_username: ekey\ndocker_password: &lt;your_entitlement_key>\ndefault_admin_password: admin\npassword_rules:\n  - '(.*)'\n\nmanagement_services:\n  # Common services\n  iam-policy-controller: enabled\n  metering: enabled\n  licensing: disabled\n  monitoring: enabled\n  nginx-ingress: enabled\n  common-web-ui: enabled\n  catalog-ui: enabled\n  mcm-kui: enabled\n  logging: enabled\n  audit-logging: disabled\n  system-healthcheck-service: disabled\n  multitenancy-enforcement: disabled\n  configmap-watcher: disabled\n# This section installs the IBM Cloud Pak for Integration Platform Navigator.\n# The navigator will be available after installation at:\n# https://ibm-icp4i-prod-integration.&lt;openshift apps domain>/\narchive_addons:\n  icp4i:\n    namespace: integration\n    repo: local-charts\n    path: icp4icontent/IBM-Cloud-Pak-for-Integration-3.0.0.tgz\n    charts:\n      - name: ibm-icp4i-prod\n        values: {}\n```\n","type":"Mdx","contentDigest":"9cf2c1e9b45b0bf1c81976e52c39fb27","counter":149,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"onprem-online","weight":400},"exports":{},"rawBody":"---\ntitle: onprem-online\nweight: 400\n---\n\n- [Introduction](#introduction)\n- [Prepare For Installation](#prepare-for-installation)\n- [Run the Integration Cloud Pak install](#run-the-integration-cloud-pak-install)\n- [Deploy Capabilities](#deploy-capabilities)\n- [Example files](#example-files)\n  - [config.yaml](#configyaml)\n\n## Introduction\n\nThis page describes all the steps on how to deploy the Integration Cloud Pak to a VMWARE onprem environment using the IBM Entitled registry. The steps below includes instructions to:\n\n1. Prepare the bastion/installation node for the installation\n2. Run the Integration Cloud Pak installer to deploy to an existing OpenShift cluster\n\n## Prepare For Installation\n\nIn many production scenarios the master nodes may not be accessed via SSH, in which case we have to create or choose a bastion node in order to proceed with the installation. This can be a worker node within the cluster or a VM set up specifically for this task.\n\n**Installer Node requirements:**\n\n- Sufficient resources of `4cpu 16GB ram ~120 GB Diskspace`\n- OpenShift CLI, which can be installed following the instructions [here on IBM Cloud](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli).\n- Docker (v2.2 is compliant and works with Openshift)\n- Kubernetes CLI kubectl\n\nOnce the CLIs are installed, check if you can login to your openshift environment:\n\n  1. Get login token from openshift console\n  2. Run the `oc login` command from a terminal shell.\n  3. You should see the cluster logged in message along with list of projects.\n  \n**Installing on Master or Infra node:**  \nThe value of the master, proxy, and management parameters is an array and can have multiple nodes. Due to a limitation from OpenShift, if you want to deploy IBM Cloud Private on any OpenShift master or infrastructure node, you must label the node as an OpenShift compute node with the following command:\n\n```bash\noc label node &lt;master node host name/infrastructure node host name> node-role.kubernetes.io/compute=true\n```\n\n## Run the Integration Cloud Pak install\n\nIBM Cloud Pak for Integration provides a single installer that installs ICP as well loads all the helm charts for integration capabilities. In this example CP4I will be installed on the master node.\n\n1. Download the Cloud Pak and extract the contents onto the bastion node. See [Pre-requisites](../pre-reqs) for guidance. It is a general recommendation to create a directory in /opt and extract into that directory:\n\n    ```bash\n    tar xf ibm-cp-int-2019.4.1-online.tar.gz --directory /opt/cp4i\n    ```\n\n    Once untarred, you can navigate to the directory where the packages was untarred to and type `tree`.  It will look like the below\n    ![Output of tree command](1.untar-cp4i.png)\n\n2. Load the images onto your local docker registry:\n\n    ```bash\n    sudo docker load -i installer_files/cluster/images/icp-inception-3.2.2.tgz\n    ```\n\n3. Change to the `installer_files/cluster/` directory. Place the cluster configuration files (kubeconfig) in the `installer_files/cluster/` directory. You can also use the following command after using oc login as admin.  Make sure your file only has one cluster context defined with in it, and that context is the location of your target cluster.\n\n    ```bash\n    oc config view > kubeconfig\n    ```\n\n4. Note down the IP addresses of OpenShift worker nodes. To get the IP addresses of the worker nodes, run:\n\n    ```bash\n    oc get nodes -o wide\n    ```\n\n5. Navigate to your cluster directory `/opt/cp4i/installer/cluster`.\n6. Edit the config.yaml with the information you have collected above. See the example at the [end of the page](#configyaml) for guidance.  \nHere are the fields to update with your respective values based on your environment:\n\n    - under cluster_nodes heading -> set the hostnames for `Master`, `Proxy` and `Management`.  For non-prdoduction systems, setting and proxy to the same host is fine. Use the short name for your nodes (e.g. compute1, compute2 etc)\n    - under storage_class -> choose your default storage class here - use `oc get sc` to get a list of available storageclasses.\n    - docker_user -> `ekey`\n    - docker_password -> set this to your entitlement key\n\n    Instructions to get your entitlement key can be found [here](https://github.ibm.com/CloudPakOpenContent/cloudpak-entitlement) if you are an IBM employee.\n\n7. Run the installer with:\n\n    ```bash\n    sudo docker run -t --net=host -e LICENSE=accept -v $(pwd):/installer/cluster:z -v /var/run:/var/run:z -v /etc/docker:/etc/docker:z --security-opt label:disable ibmcom/icp-inception-amd64:3.2.2 addon -vvv | tee install.log\n    ```\n\n8. If the namespaces for the different capabilities did not create you can create them manually using the scripts in `installer_files/cluster/resources` e.g. ace.yaml, apic.yaml.  Simply run each script using this syntax:\n\n    ```bash\n    oc create -f &lt;scriptname>.yaml\n    ```\n\n9. Once the process is complete, you will need to create your `ibm_entitlement_key` secrets in all of the main component namespaces.  You can accomplish this by running the `create_secrets.sh` script. Before doing so, export these two variables:\n\n    ```bash\n    export DOCKER_REGISTRY_USER=ekey\n    export DOCKER_REGISTRY_PASS=&lt;your entitlement key>\n    ```\n\n10. It will be helpful to understand what your proxy node address is, as it will be referenced several time when deploying the individual capabilities.  Run this command and take note of its output:\n\n    ```bash\n    oc get configmap -n kube-public ibmcloud-cluster-info -o=jsonpath=\"{.data.proxy_address}\"`\n    ```\n\n## Deploy Capabilities\n\nIt is recommended that you install the Tracing capability first\n\n- [Tracing](../deploy-tracing)\n- [App Connect](../deploy-integration)\n- [API Connect](../deploy-api-mgmt)\n- [MQ](../deploy-queue-manager)\n- [Event Streams](../deploy-eventstreams)\n- [Aspera](../deploy-fast-file-transfer)\n- [DataPower](../deploy-secure-gateway)\n- [Asset Repository](../deploy-asset-repo)\n\n## Example files\n\nThis section contains examples of files you will be using throughout the installation. Refer to them for guidance on how to populate your own version of the files.\n\n### config.yaml\n\n```yaml\n# Nodes selected to run common services components.\n#\n# The value of the master, proxy, and management parameters is an array,\n# by providing multiple nodes the common services will be configured in\n# a high availability configuration.\n#\n# It is recommended to install the components onto one or more openshift\n# worker nodes. The master, proxy, and management components can all share\n# the same node or set of nodes.\ncluster_nodes:\n  master:\n    - compute1\n  proxy:\n    - compute1\n  management:\n    - compute2\n\n# This storage class is used to store persistent data for the common services\n# components\nstorage_class: csi-cephfs\n\n## You can set a different storage class for storing log data.\n## By default it will use the value of storage_class.\n# elasticsearch_storage_class:\n\n# These settings enable the installer to install common services from the IBM\n# Entitled Registry. You will need to supply your entitlement key as described\n# at https://github.com/ibm/charts\nprivate_registry_enabled: true\nimage_repo: cp.icr.io/cp/icp-foundation\ndocker_username: ekey\ndocker_password: &lt;your_entitlement_key>\ndefault_admin_password: admin\npassword_rules:\n  - '(.*)'\n\nmanagement_services:\n  # Common services\n  iam-policy-controller: enabled\n  metering: enabled\n  licensing: disabled\n  monitoring: enabled\n  nginx-ingress: enabled\n  common-web-ui: enabled\n  catalog-ui: enabled\n  mcm-kui: enabled\n  logging: enabled\n  audit-logging: disabled\n  system-healthcheck-service: disabled\n  multitenancy-enforcement: disabled\n  configmap-watcher: disabled\n# This section installs the IBM Cloud Pak for Integration Platform Navigator.\n# The navigator will be available after installation at:\n# https://ibm-icp4i-prod-integration.&lt;openshift apps domain>/\narchive_addons:\n  icp4i:\n    namespace: integration\n    repo: local-charts\n    path: icp4icontent/IBM-Cloud-Pak-for-Integration-3.0.0.tgz\n    charts:\n      - name: ibm-icp4i-prod\n        values: {}\n```\n","fileAbsolutePath":"/Users/Shared/VBDData/programs/cloudpak8s/src/pages/content/integration/onprem-online.md"}}}}