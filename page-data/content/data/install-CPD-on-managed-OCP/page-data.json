{"componentChunkName":"component---src-pages-content-data-install-cpd-on-managed-ocp-md","path":"/content/data/install-CPD-on-managed-OCP/","result":{"pageContext":{"frontmatter":{"title":"Install CPD on Managed OCP in IBM Cloud","weight":400},"relativePagePath":"/content/data/install-CPD-on-managed-OCP.md","titleType":"page","MdxNode":{"id":"7d486040-f041-58c3-b34f-768b63a876da","children":[],"parent":"be195b27-f986-5104-a12b-6e6f62212b92","internal":{"content":"---\ntitle: Install CPD on Managed OCP in IBM Cloud\nweight: 400\n---\n\n## Create installation script and run it in terminal.\n\n1 Copy the content of install-cp4data.sh from IBM Knowledge Center: \n\n  [Installing Cloud Pak for Data on managed Red Hat OpenShift on IBM Cloud](https://www.ibm.com/support/knowledgecenter/en/SSQNUZ_2.1.0/com.ibm.icpdata.doc/zen/install/openshift-softlayer.html)\n  \n2 Change the content of DOCKER-REGISTRY related parameters and here is an example in IBM Cloud environment (password is not provided for security reason): \n\n    DOCKER_REGISTRY=\"us.icr.io/release2_1_0_1_base\"\n    DOCKER_REGISTRY_USER=\"iamapikey\"\n    DOCKER_REGISTRY_PASS=\"&lt;&lt;inform here the docker password>>\"\n\n 3 Run the shell script:\n \n \n    ./install-cp4data.sh zen\n    \n    \n\n## Start CPD deployment in Openshift webconsole.\n\n  1 In the OpenShift container platform, go to the created namespace.\n  \n  {%\n  include figure.html\n  src=\"/assets/img/cp4d/ocp-name-space.jpg\"\n  alt=\"Go to Namespace\"\n  caption=\"Go to Namespace\"\n%}\n\n  2 Click Select from Project. Alternatively, you can go to the Catalog Menu and select Cloud Pak for Data.\n\n{%\n  include figure.html\n  src=\"/assets/img/cp4d/select-from-project.jpg\"\n  alt=\"Select From Project\"\n  caption=\"Select From Project\"\n  %}\n  \n  3 In the Selection window, click Cloud Pak for Data. Click Next.\n\n{%\n  include figure.html\n  src=\"/assets/img/cp4d/select-cpd.jpg\"\n  alt=\"Selection Window\"\n  caption=\"Selection Window\"\n  %}\n  \n  4 In the Information window, click Next.\n  \n  {%\n  include figure.html\n  src=\"/assets/img/cp4d/information-window-next.jpg\"\n  alt=\"Information Window\"\n  caption=\"Information Window\"\n  %}\n  \n  5 In the Configuration window, fill in your cluster environment information. Ensure the StorageClass is correct. Click Create. \n  \n  {%\n  include figure.html\n  src=\"/assets/img/cp4d/configuration-window.jpg\"\n  alt=\"Configuration Window\"\n  caption=\"Configuration Window\"\n  %}\n  \n  Here is an example StorageClass:\n  \n       ibmc-file-gold\n  \n  6 In the Results window, click Continue on the project overview to see the status.\n\n{%\n  include figure.html\n  src=\"/assets/img/cp4d/results-window.jpg\"\n  alt=\"Results Window\"\n  caption=\"Results Window\"\n  %}\n  \n\n## Monitor the deployment process.\n\n  1 Keep monitoring the log of deployment. In a correct order, first step will be deploying pod: \"cp4data-installer-1-sp28b\" and then you can view the log of the this pod to monitor the process the all the other components of CPD.\n  \n  2 Use command \"oc get pods\" to check the status in the terminal. \n  \n  3 it may take up to 2 hours to finish the deployment and finally you will have 83 pods running!\n  \n     qijuns-mbp:OCP qijunwang$ oc get pods -n zen | wc -l\n      \n     84\n     \n     qijuns-mbp:OCP qijunwang$ oc get pods -n zen | grep -v Running | grep -v Completed\n     \n     NAME                   READY     STATUS    RESTARTS   AGE\n      \n     qijuns-mbp:OCP qijunwang$ \n\n  \n## Known issues and resolutions.\n\n   1  Failed to pull images from Docker REGISTRY.\n   \n   Log of the error:\n   \n     2:08:36 PM     cp4data-installer-1-87cmn     Pod     Warning     Failed      Error: ImagePullBackOff\n     3 times in the last minute\n     2:08:36 PM     cp4data-installer-1-87cmn     Pod     Normal     Back-off      Back-off pulling image \"cp.stg.icr.io/cp /cp4d/cp4d-installer:v1\"\n     3 times in the last minute\n     2:08:22 PM     cp4data-installer-1-87cmn     Pod     Warning     Failed      Failed to pull image \"cp.stg.icr.io/cp/cp4d/cp4d-installer:v1\": rpc error: code = Unknown desc = unable to retrieve auth token: invalid username/password\n     3 times in the last minute\n     2:08:22 PM     cp4data-installer-1-87cmn     Pod     Warning     Failed      Error: ErrImagePull\n     3 times in the last minute\n     2:08:22 PM     cp4data-installer-1-87cmn     Pod     Normal     Pulling      pulling image \"cp.stg.icr.io/cp/cp4d/cp4d-installer:v1\"\n     3 times in the last minute\n    2:07:27 PM     cp4data-installer-1-87cmn     Pod     Warning     Failed Create Pod Sand Box      Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_cp4data-installer-1-87cmn_zen_d412daf1-cf3e-11e9-bd0c-2a0a29181891_0(be9ae5031560dc694d70d4d0e41c52e1b95832493b99efef92ce168251d80581): context deadline exceeded\n\n\n  Resolution:\n  \n     Change the Docker Registry URL to \"us.icr.io/release2_1_0_1_base\".\n     \n  2  Stuck by \"services \"ibm-nginx-svc\" not found\".\n     \n   Log of the error: \n     \n     installctl/cmd.glob..func3(0x2978720, 0xc0001a22a0, 0x0, 0xe)\n    /root/go/src/github.ibm.com/privatecloud/installctl/cmd/deploy.go:39 +0x20\n    github.com/spf13/cobra.(*Command).execute(0x2978720, 0xc0001a21c0, 0xe, 0xe, 0x2978720, 0xc0001a21c0)\n    /root/go/pkg/mod/github.com/spf13/cobra@v0.0.0-20190805155617-b80588d523ec/command.go:833 +0x2ae\n    github.com/spf13/cobra.(*Command).ExecuteC(0x2978c20, 0x4, 0x17e2509, 0x32)\n    /root/go/pkg/mod/github.com/spf13/cobra@v0.0.0-20190805155617-b80588d523ec/command.go:917 +0x2fc\n    github.com/spf13/cobra.(*Command).Execute(...)\n    /root/go/pkg/mod/github.com/spf13/cobra@v0.0.0-20190805155617-b80588d523ec/command.go:867\n    installctl/cmd.Execute()\n    /root/go/src/github.ibm.com/privatecloud/installctl/cmd/root.go:38 +0x32\n    main.main()\n    /root/go/src/github.ibm.com/privatecloud/installctl/main.go:34 +0x20\n    No resources found.\n    Error from server (NotFound): services \"ibm-nginx-svc\" not found\n    \n  Resolution:\n  \n    Restart the pod: \"cp4data-installer\" by \"oc delete\" in terminal or Openshift webconsole.\n    \n 3 Stuck by \"unbound PersistentVolumeClaims\".\n     \n   Log of the error: \n   \n     time=\"2019-09-04T19:35:03Z\" level=info msg=\"Release present on the cluster in DEPLOYED state. Upgrading...\"\n     time=\"2019-09-04T19:35:03Z\" level=info msg=\"Upgrading release zen-0005-boot\"\n     time=\"2019-09-04T19:35:04Z\" level=info msg=\"Release Deployment Status: zen-0005-boot - Deploy: -1/1 - Pod: 0/0 - Job: 0/0 - Pvc: 0/1\"\n     time=\"2019-09-04T19:35:05Z\" level=info msg=\"Release Deployment Status: zen-0005-boot - Deploy: 0/1 - Pod: 0/1 - Job: 0/0 - Pvc: 0/1\"\n     no error or warning, but no progress.\n    Events:\n     Type     Reason            Age                     From               Message\n     ----     ------            ----                    ----               -------\n     Warning  FailedScheduling  2m15s (x25 over 3m18s)  default-scheduler  pod has unbound PersistentVolumeClaims (repeated 3 times)\n     \n   Resolution:\n   \n    Change the storageclass to \"ibmc-file-gold\" and restart the deployment.\n    \n  4 Stuck by wrong report information\n  \n  Log of the error:\n  \n    time=\"2019-09-04T20:11:23Z\" level=info msg=\"Values override output: global:\\n  architecture: amd64\\n  baseInstaller: false\\n  ibmProduct: zen\\n  userHomePVC:\\n    persistence:\\n      existingClaimName: \\\"\\\"\\n      size: 100Gi\\n  virtualIP: \\\"\\\"\\nimagemgmt:\\n  mgmtPlatform: openshift\\n  nodeSelector:\\n    compute: \\\"true\\\"\\nnginxRepo:\\n  resolver: kubernetes.default\\nusermgmt:\\n  showK8sMgmt: false\\nzenCoreApi:\\n  noTls: true\\n  tillerNamespace: zen\\nzenProxy:\\n  serviceType: ClusterIP\\nzenRedis:\\n  image:\\n    tag: '#REDIS_TAG#'\\n\"\n    time=\"2019-09-04T20:11:23Z\" level=info msg=\"Installing release zen-0015-setup\"\n    time=\"2019-09-04T20:11:57Z\" level=panic msg=\"Could not get the status of the release. Reason: rpc error: code = Unknown desc = getting deployed release \\\"zen-0015-setup\\\": release: \\\"zen-0015-setup\\\" not found\"\n    panic: (*logrus.Entry) (0x176bd40,0xc00040afc0)goroutine 3405 [running]:\n    github.com/sirupsen/logrus.Entry.log(0xc00009c240, 0xc0004e52c0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\n    /root/go/pkg/mod/github.com/sirupsen/logrus@v1.2.0/entry.go:216 +0x2ce\n    \n  Resolution:\n  \n    Delete the installation pod.\n    \n 5 Solr, Kafka and cassendra pods failed with similar message.\n \n  Log of the error:\n  \n    [2019-09-04 21:38:20,833] ERROR Failed to create or validate data directory /var/lib/kafka/data (kafka.server.LogDirFailureChannel)\n    java.io.IOException: Failed to create data directory /var/lib/kafka/data\n    at kafka.log.LogManager$$anonfun$createAndValidateLogDirs$1.apply(LogManager.scala:158)\n    at kafka.log.LogManager$$anonfun$createAndValidateLogDirs$1.apply(LogManager.scala:149)\n    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n    at kafka.log.LogManager.createAndValidateLogDirs(LogManager.scala:149)\n    at kafka.log.LogManager.&lt;init>(LogManager.scala:80)\n    at kafka.log.LogManager$.apply(LogManager.scala:990)\n    at kafka.server.KafkaServer.startup(KafkaServer.scala:237)\n    at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38)\n    at kafka.Kafka$.main(Kafka.scala:75)\n    at kafka.Kafka.main(Kafka.scala)\n    [2019-09-04 21:38:20,839] ERROR Shutdown broker because none of the specified log dirs from /var/lib/kafka/data can be created or validated (kafka.log.LogManager)\n    \n    \n  Resolution:\n  \n  Copy and run the follwing shell script and then run it by \"./iisee-fix.sh zen\".\n  \n  [iisee-fix.sh](https://github.ibm.com/CASE/cloudpak-onboard-residency/blob/gh-pages/_content/data/iisee-fix.sh)\n  \n \n \n     \n","type":"Mdx","contentDigest":"8871583394764bf9783147f173357b99","counter":165,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Install CPD on Managed OCP in IBM Cloud","weight":400},"exports":{},"rawBody":"---\ntitle: Install CPD on Managed OCP in IBM Cloud\nweight: 400\n---\n\n## Create installation script and run it in terminal.\n\n1 Copy the content of install-cp4data.sh from IBM Knowledge Center: \n\n  [Installing Cloud Pak for Data on managed Red Hat OpenShift on IBM Cloud](https://www.ibm.com/support/knowledgecenter/en/SSQNUZ_2.1.0/com.ibm.icpdata.doc/zen/install/openshift-softlayer.html)\n  \n2 Change the content of DOCKER-REGISTRY related parameters and here is an example in IBM Cloud environment (password is not provided for security reason): \n\n    DOCKER_REGISTRY=\"us.icr.io/release2_1_0_1_base\"\n    DOCKER_REGISTRY_USER=\"iamapikey\"\n    DOCKER_REGISTRY_PASS=\"&lt;&lt;inform here the docker password>>\"\n\n 3 Run the shell script:\n \n \n    ./install-cp4data.sh zen\n    \n    \n\n## Start CPD deployment in Openshift webconsole.\n\n  1 In the OpenShift container platform, go to the created namespace.\n  \n  {%\n  include figure.html\n  src=\"/assets/img/cp4d/ocp-name-space.jpg\"\n  alt=\"Go to Namespace\"\n  caption=\"Go to Namespace\"\n%}\n\n  2 Click Select from Project. Alternatively, you can go to the Catalog Menu and select Cloud Pak for Data.\n\n{%\n  include figure.html\n  src=\"/assets/img/cp4d/select-from-project.jpg\"\n  alt=\"Select From Project\"\n  caption=\"Select From Project\"\n  %}\n  \n  3 In the Selection window, click Cloud Pak for Data. Click Next.\n\n{%\n  include figure.html\n  src=\"/assets/img/cp4d/select-cpd.jpg\"\n  alt=\"Selection Window\"\n  caption=\"Selection Window\"\n  %}\n  \n  4 In the Information window, click Next.\n  \n  {%\n  include figure.html\n  src=\"/assets/img/cp4d/information-window-next.jpg\"\n  alt=\"Information Window\"\n  caption=\"Information Window\"\n  %}\n  \n  5 In the Configuration window, fill in your cluster environment information. Ensure the StorageClass is correct. Click Create. \n  \n  {%\n  include figure.html\n  src=\"/assets/img/cp4d/configuration-window.jpg\"\n  alt=\"Configuration Window\"\n  caption=\"Configuration Window\"\n  %}\n  \n  Here is an example StorageClass:\n  \n       ibmc-file-gold\n  \n  6 In the Results window, click Continue on the project overview to see the status.\n\n{%\n  include figure.html\n  src=\"/assets/img/cp4d/results-window.jpg\"\n  alt=\"Results Window\"\n  caption=\"Results Window\"\n  %}\n  \n\n## Monitor the deployment process.\n\n  1 Keep monitoring the log of deployment. In a correct order, first step will be deploying pod: \"cp4data-installer-1-sp28b\" and then you can view the log of the this pod to monitor the process the all the other components of CPD.\n  \n  2 Use command \"oc get pods\" to check the status in the terminal. \n  \n  3 it may take up to 2 hours to finish the deployment and finally you will have 83 pods running!\n  \n     qijuns-mbp:OCP qijunwang$ oc get pods -n zen | wc -l\n      \n     84\n     \n     qijuns-mbp:OCP qijunwang$ oc get pods -n zen | grep -v Running | grep -v Completed\n     \n     NAME                   READY     STATUS    RESTARTS   AGE\n      \n     qijuns-mbp:OCP qijunwang$ \n\n  \n## Known issues and resolutions.\n\n   1  Failed to pull images from Docker REGISTRY.\n   \n   Log of the error:\n   \n     2:08:36 PM     cp4data-installer-1-87cmn     Pod     Warning     Failed      Error: ImagePullBackOff\n     3 times in the last minute\n     2:08:36 PM     cp4data-installer-1-87cmn     Pod     Normal     Back-off      Back-off pulling image \"cp.stg.icr.io/cp /cp4d/cp4d-installer:v1\"\n     3 times in the last minute\n     2:08:22 PM     cp4data-installer-1-87cmn     Pod     Warning     Failed      Failed to pull image \"cp.stg.icr.io/cp/cp4d/cp4d-installer:v1\": rpc error: code = Unknown desc = unable to retrieve auth token: invalid username/password\n     3 times in the last minute\n     2:08:22 PM     cp4data-installer-1-87cmn     Pod     Warning     Failed      Error: ErrImagePull\n     3 times in the last minute\n     2:08:22 PM     cp4data-installer-1-87cmn     Pod     Normal     Pulling      pulling image \"cp.stg.icr.io/cp/cp4d/cp4d-installer:v1\"\n     3 times in the last minute\n    2:07:27 PM     cp4data-installer-1-87cmn     Pod     Warning     Failed Create Pod Sand Box      Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_cp4data-installer-1-87cmn_zen_d412daf1-cf3e-11e9-bd0c-2a0a29181891_0(be9ae5031560dc694d70d4d0e41c52e1b95832493b99efef92ce168251d80581): context deadline exceeded\n\n\n  Resolution:\n  \n     Change the Docker Registry URL to \"us.icr.io/release2_1_0_1_base\".\n     \n  2  Stuck by \"services \"ibm-nginx-svc\" not found\".\n     \n   Log of the error: \n     \n     installctl/cmd.glob..func3(0x2978720, 0xc0001a22a0, 0x0, 0xe)\n    /root/go/src/github.ibm.com/privatecloud/installctl/cmd/deploy.go:39 +0x20\n    github.com/spf13/cobra.(*Command).execute(0x2978720, 0xc0001a21c0, 0xe, 0xe, 0x2978720, 0xc0001a21c0)\n    /root/go/pkg/mod/github.com/spf13/cobra@v0.0.0-20190805155617-b80588d523ec/command.go:833 +0x2ae\n    github.com/spf13/cobra.(*Command).ExecuteC(0x2978c20, 0x4, 0x17e2509, 0x32)\n    /root/go/pkg/mod/github.com/spf13/cobra@v0.0.0-20190805155617-b80588d523ec/command.go:917 +0x2fc\n    github.com/spf13/cobra.(*Command).Execute(...)\n    /root/go/pkg/mod/github.com/spf13/cobra@v0.0.0-20190805155617-b80588d523ec/command.go:867\n    installctl/cmd.Execute()\n    /root/go/src/github.ibm.com/privatecloud/installctl/cmd/root.go:38 +0x32\n    main.main()\n    /root/go/src/github.ibm.com/privatecloud/installctl/main.go:34 +0x20\n    No resources found.\n    Error from server (NotFound): services \"ibm-nginx-svc\" not found\n    \n  Resolution:\n  \n    Restart the pod: \"cp4data-installer\" by \"oc delete\" in terminal or Openshift webconsole.\n    \n 3 Stuck by \"unbound PersistentVolumeClaims\".\n     \n   Log of the error: \n   \n     time=\"2019-09-04T19:35:03Z\" level=info msg=\"Release present on the cluster in DEPLOYED state. Upgrading...\"\n     time=\"2019-09-04T19:35:03Z\" level=info msg=\"Upgrading release zen-0005-boot\"\n     time=\"2019-09-04T19:35:04Z\" level=info msg=\"Release Deployment Status: zen-0005-boot - Deploy: -1/1 - Pod: 0/0 - Job: 0/0 - Pvc: 0/1\"\n     time=\"2019-09-04T19:35:05Z\" level=info msg=\"Release Deployment Status: zen-0005-boot - Deploy: 0/1 - Pod: 0/1 - Job: 0/0 - Pvc: 0/1\"\n     no error or warning, but no progress.\n    Events:\n     Type     Reason            Age                     From               Message\n     ----     ------            ----                    ----               -------\n     Warning  FailedScheduling  2m15s (x25 over 3m18s)  default-scheduler  pod has unbound PersistentVolumeClaims (repeated 3 times)\n     \n   Resolution:\n   \n    Change the storageclass to \"ibmc-file-gold\" and restart the deployment.\n    \n  4 Stuck by wrong report information\n  \n  Log of the error:\n  \n    time=\"2019-09-04T20:11:23Z\" level=info msg=\"Values override output: global:\\n  architecture: amd64\\n  baseInstaller: false\\n  ibmProduct: zen\\n  userHomePVC:\\n    persistence:\\n      existingClaimName: \\\"\\\"\\n      size: 100Gi\\n  virtualIP: \\\"\\\"\\nimagemgmt:\\n  mgmtPlatform: openshift\\n  nodeSelector:\\n    compute: \\\"true\\\"\\nnginxRepo:\\n  resolver: kubernetes.default\\nusermgmt:\\n  showK8sMgmt: false\\nzenCoreApi:\\n  noTls: true\\n  tillerNamespace: zen\\nzenProxy:\\n  serviceType: ClusterIP\\nzenRedis:\\n  image:\\n    tag: '#REDIS_TAG#'\\n\"\n    time=\"2019-09-04T20:11:23Z\" level=info msg=\"Installing release zen-0015-setup\"\n    time=\"2019-09-04T20:11:57Z\" level=panic msg=\"Could not get the status of the release. Reason: rpc error: code = Unknown desc = getting deployed release \\\"zen-0015-setup\\\": release: \\\"zen-0015-setup\\\" not found\"\n    panic: (*logrus.Entry) (0x176bd40,0xc00040afc0)goroutine 3405 [running]:\n    github.com/sirupsen/logrus.Entry.log(0xc00009c240, 0xc0004e52c0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\n    /root/go/pkg/mod/github.com/sirupsen/logrus@v1.2.0/entry.go:216 +0x2ce\n    \n  Resolution:\n  \n    Delete the installation pod.\n    \n 5 Solr, Kafka and cassendra pods failed with similar message.\n \n  Log of the error:\n  \n    [2019-09-04 21:38:20,833] ERROR Failed to create or validate data directory /var/lib/kafka/data (kafka.server.LogDirFailureChannel)\n    java.io.IOException: Failed to create data directory /var/lib/kafka/data\n    at kafka.log.LogManager$$anonfun$createAndValidateLogDirs$1.apply(LogManager.scala:158)\n    at kafka.log.LogManager$$anonfun$createAndValidateLogDirs$1.apply(LogManager.scala:149)\n    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n    at kafka.log.LogManager.createAndValidateLogDirs(LogManager.scala:149)\n    at kafka.log.LogManager.&lt;init>(LogManager.scala:80)\n    at kafka.log.LogManager$.apply(LogManager.scala:990)\n    at kafka.server.KafkaServer.startup(KafkaServer.scala:237)\n    at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38)\n    at kafka.Kafka$.main(Kafka.scala:75)\n    at kafka.Kafka.main(Kafka.scala)\n    [2019-09-04 21:38:20,839] ERROR Shutdown broker because none of the specified log dirs from /var/lib/kafka/data can be created or validated (kafka.log.LogManager)\n    \n    \n  Resolution:\n  \n  Copy and run the follwing shell script and then run it by \"./iisee-fix.sh zen\".\n  \n  [iisee-fix.sh](https://github.ibm.com/CASE/cloudpak-onboard-residency/blob/gh-pages/_content/data/iisee-fix.sh)\n  \n \n \n     \n","fileAbsolutePath":"/home/travis/build/vbudi000/cloudpak8s/src/pages/content/data/install-CPD-on-managed-OCP.md"}}}}