{"componentChunkName":"component---src-pages-content-ocp-openshift-3-terraform-installation-vsphere-md","path":"/_content/ocp/openshift3_terraform_installation_vsphere/","result":{"pageContext":{"frontmatter":{},"relativePagePath":"/_content/ocp/openshift3_terraform_installation_vsphere.md","titleType":"page","MdxNode":{"id":"4bbd6ddb-615f-5261-bd53-e7b747ed40f9","children":[],"parent":"d20e060a-ba36-59a8-acf6-05d2057b7fe8","internal":{"content":"## Openshift 3.11 Terraform-based installation on vSphere\n\nTo install OpenShift 3.11 on VMware using Terraform, you need the following:\n\n- Terraform 0.12.x installation \n- VMware vSphere with API access \n- RedHat Network subscription for OpenShift\n- Sizing information for the cluster\n- DNS, subnet, gateway and available IPs for all cluster nodes\n\nThe first thing to install OpenShift 3.11 on a VMware information is to load the GIT repo for the VMware example:\n\n```bash\ngit clone https://github.com/ibm-cloud-architecture/terraform-openshift3-vmware-example\n```\n\nThe files that you get from the GIT repo are:\n\n- `variables.tf`\n- `main.tf`\n- `infrastructure.tf`\n- `loadbalancer.tf`\n- `dns.tf`\n- `certs.tf`\n- `output.tf`\n\nIn the deployment, you must configure and review each of the `.tf` files for your infrastructure, and create and configure a `terraform.tfvars`.  We have attempted to separate them by the concerns in the filename.\n\nEach `*.tf` contains modules that can be invoked for the deployment that you may not needed depending on your configuration.  For example, if high-availability is not required, the `loadbalancer.tf` file can be deleted and related variables can be removed from the other files.\n\nOne thing that you should do is to decide on how you want to manage your DNS. Whether you are using CloudFlare with LetsEncrypt, an RFC2136 compliant Dynamic DNS, nip.io or others. If you choose others, then it is very advisable to perform the `/etc/hosts` file customization to ensure that all nodes are recognized properly.\n\n### Configuring Terraform\n\n#### Accessing vSphere\n\nOperation with the VMware infratructure is performed through the vSphere API. This section of the variables represents the resources that exists in the vSphere and must be identified to create the infrastructure. The snippet is below.\n\n```\n#######################################\n##### vSphere Access Credentials ######\n#######################################\nvsphere_server = \"vsphere-server.my-domain.com\"\n\n# Set username/password as environment variables VSPHERE_USER and VSPHERE_PASSWORD\n\n##############################################\n##### vSphere deployment specifications ######\n##############################################\n# Following resources must exist in vSphere\nvsphere_datacenter = \"CSPLAB\"\nvsphere_cluster = \"Sandbox\"\nvsphere_resource_pool = \"test-pool\"\ndatastore_cluster = \"SANDBOX_TIER4\"\n```\n\nIn the resources view, these are the vsphere hierarchy that must be identified to create the VM: \n\n![vsphere resource](assets/ocp/vsphere_resources.png)\n\nThe disk images for VMs are stored in either a Datastore or in a larger environments, a Datastore cluster. You can specify either of this, but not both, the example above is using the `datastore_cluster` the option is using the `datastore` option. Go to the datastore tab and the following is the illustration: \n  \n![datastore](assets/ocp/vsphere_datastore.png)\n\nNote that these names are case sensitive.\n\n#### vSphere storage class information\n\nThese values specify the vSphere username and password to access the datastore.  When Openshift is installed, a storageclass named `vsphere-standard` will be used to create block volumes on the datastore specified using the vSphere user and password.\n\n```\n# for the vsphere-standard storage class\nvsphere_storage_username = \"&lt;storageuser>\"\nvsphere_storage_password = \"&lt;storagepassword>\"\nvsphere_storage_datastore = \"ds01\"\n```\n\nPlease see the following [link](https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/vcp-roles.html) for the required permissions needed to by the storage user.\n\n\n#### Template \n\nThis information is needed also from the vSphere. OpenShift requires RedHat Enterprise Linux 7.4 or later VM template. You also should supply the credential to access the VMs that are created from the template. The credential can be using `ssh_user` and either one of `ssh_password` or `ssh_private_key_file`. The hostname prefix is used to prefix both the VM names and the hostname to be created in the DNS. Note that these names would have a random 8 hexadecimal characters to make sure that the hosts are unique.\n\n```\ntemplate = \"rhel-7.6-template\"\n# SSH username and private key to connect to VM template, has passwordless sudo access\nssh_user = \"virtuser\"\nssh_password = \"&lt;mypassword>\"\nssh_private_key_file = \"~/.ssh/id_rsa\"\n\n# MUST consist of only lower case alphanumeric characters and '-'\nhostname_prefix = \"ocp311\"\n```\n\nPlease see the following [link](ocp/RHEL_template) for information about template preparation.\n\n#### vSphere folder \n\nThe folder defined should not exist as the installation will create them, this folder may be a path on which the last qualifier will be created. \n\n```\n# vSphere Folder to provision the new VMs in, will be created\nfolder = \"openshift311-folder\"\n```\n\n#### Redhat account information\n\nThese are RedHat account to get either the RedHat Network subscription and getting the images for OpenShift. You can use the same username and password, but it is recommended that you create a RedHat service account for the image registry (see [these instructions](https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/configuring_clusters/install-config-configuring-red-hat-registry)). The RedHat subscription pool id can be retrieved from [this page](https://access.redhat.com/management/subscriptions?type=active) and select the subscription ID that you wanted to use.\n\n```\n# it's best to use a service account for these\nimage_registry_username = \"&lt;registry.redhat.io service account username>\"\nimage_registry_password = \"&lt;registry.redhat.io service acocunt password>\"\n\nrhn_username = \"&lt;rhn username>\"\nrhn_password = \"&lt;rhn password>\"\nrhn_poolid = \"&lt;rhn pool id>\"\n```\n\n#### Networking settings\n\nNetworking variables must be configured for the VMs. You may provide values for configuring both a private and public networks. see also ![network](assets/ocp/network.png). \n\nThe public network parameters are optional; if specified, it will place the bastion node on the public network.  In these scenarios you may want to stand up two load balancers on both the private and public networks to expose client traffic.\n\nAs a note, the example private network above would generate the first IP of `192.168.101.11` as the mask + offset + 1 gives you that address. You would need 4 addresses in the public network and the number of nodes + 1 for the private network. If you happen to have only a single flat network (ie the public and private network are the same network) then you would have to code the offset at least with a difference of 4.  \n\n```\n##### Network #####\nprivate_network_label = \"private_network\"\nprivate_staticipblock = \"192.168.101.0/24\"\nprivate_staticipblock_offset = 10           # IP assignment starts at 192.168.101.11\nprivate_netmask = \"24\"\nprivate_gateway = \"192.168.101.1\"\nprivate_domain = \"internal-network.local\"\nprivate_dns_servers = [ \"192.168.101.2\" ]\n```\n\nOptionally, if you want to place the cluster on multiple networks, the cluster nodes where the pod overlay network is set up can be a completely private network, while the bastion host and load balancers can be placed on an external network.\n\n```\npublic_network_label = \"external_network\"\npublic_staticipblock = \"10.30.65.0/24\"\npublic_staticipblock_offset = 30            # ips - [ 10.30.65.31, 10.30.65.32, 10.30.65.33 ]\npublic_netmask = \"24\"\npublic_gateway = \"10.30.65.1\"\npublic_domain = \"my-public-domain.com\"\npublic_dns_servers = [ \"1.1.1.1\" ]\n```\n\nThis Terraform template also supports non-contiguous ip addresses. You can specify specific node IP addresses. In this example, the terraform will provision two worker nodes, at `.13` and `.14` while there will be 3 storage nodes, at `.15`, `.16`, `.17`. \n\n```\n##### Network #####\nprivate_network_label = \"private_network\"\nbastion_private_ip = [\"192.168.0.10\"]\nmaster_private_ip = [\"192.168.0.11\"]\ninfra_private_ip = [\"192.168.0.12\"]\nworker_private_ip = [\"192.168.0.13\", \"192.168.0.14\"]\nstorage_private_ip = [\"192.168.0.15\", \"192.168.0.16\", \"192.168.0.17\"]\n\nprivate_netmask = \"24\"\nprivate_gateway = \"192.168.0.1\"\nprivate_domain = \"my-private-domain.local\"\nprivate_dns_servers = [ \"192.168.0.1\" ]\n```\n\n#### DNS settings\n\nThe `master_cname` and `app_cname` may be manually defined in the DNS for accessing the console and application routes, or added automatically using one of the DNS modules.  The `master_cname` is a CNAME record in DNS pointing at the master node or a load balancer distributing traffic to the master nodes.  The `app_cname` is a wildcard CNAME record in DNS pointing at the infra node or a load balancer distributing traffic to the infra nodes.\n\n```\n# these were added to my public DNS, the app_cname is a wildcard\nmaster_cname = \"ocp-console.my-public-domain.com\"\napp_cname = \"ocp-apps.my-public-domain.com\"\n```\n\n#### Nodes definition and sizing\n\nthis section defines the number of nodes for each kinds and how large is the vcpu, memory and disk sizes are. The disk size minimal is determined by the template, it must be the same or larger than the template that you start on. \n\n```\n# node definitions\nmaster = {\n  nodes = \"3\"\n  vcpu = \"8\"\n  memory = \"16384\"\n\n  disk_size = \"100\"\n  docker_disk_size = \"100\"\n  thin_provisioned = \"true\"\n  keep_disk_on_remove = false\n  eagerly_scrub = false\n}\n\ninfra = {\n  nodes = \"3\"\n  vcpu = \"8\"\n  memory = \"32768\"\n\n  disk_size = \"100\"\n  docker_disk_size = \"100\"\n  thin_provisioned = \"true\"\n  keep_disk_on_remove = false\n  eagerly_scrub = false\n}\n\nworker = {\n  nodes = \"3\"\n  vcpu = \"8\"\n  memory = \"32768\"\n\n  disk_size = \"100\"\n  docker_disk_size = \"100\"\n  thin_provisioned = \"true\"\n  keep_disk_on_remove = false\n  eagerly_scrub = false\n}\n\nstorage = {\n  nodes = \"3\"\n  vcpu = \"4\"\n  memory = \"16384\"\n\n  disk_size = \"100\"\n  docker_disk_size = \"100\"\n  gluster_num_disks = \"1\"\n  gluster_disk_size = \"200\"\n  thin_provisioned = \"true\"\n  keep_disk_on_remove = false\n  eagerly_scrub = false\n}\n```\n\n#### vSphere Credentials\n\nOnce you have all the variables customized, you also need to setup some environment variables to store your credentials:\n\n- `VSPHERE_USER` and `VSPHERE_PASSWORD`: user ID and password to access the VMware vSphere environment\n\nUse the following command:\n\n```bash\nexport VSPHERE_USER=&lt;user>\nexport VSPHERE_PASSWORD=&lt;password>\n```\n\n### Provision the environment using Terraform\n\n- Initialize Terraform environments (pulling in modules and plugins)\n\n  ```bash\n  terraform init\n  ```\n\n- use plan to see what terraform will do and validate the variables are correct:\n\n  ```bash\n  terraform plan\n  ```\n\n- Provision the environment\n\n  ```bash\n  terraform apply -auto-approve\n  ```\n\nThe result should gives you the OpenShift 3.11 environment.\n\nYou should be able to access the environment using the URL of `https://ocp-console.&lt;domain>` and logging in initially as `admin` with the password of `admin`.  Once additional set up of the environment is complete, you may want to configure a different identity provider for Openshift by following the [documentation](https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html).\n","type":"Mdx","contentDigest":"f1b58b1742779e738a7829bcdc73e42e","counter":150,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":""},"exports":{},"rawBody":"## Openshift 3.11 Terraform-based installation on vSphere\n\nTo install OpenShift 3.11 on VMware using Terraform, you need the following:\n\n- Terraform 0.12.x installation \n- VMware vSphere with API access \n- RedHat Network subscription for OpenShift\n- Sizing information for the cluster\n- DNS, subnet, gateway and available IPs for all cluster nodes\n\nThe first thing to install OpenShift 3.11 on a VMware information is to load the GIT repo for the VMware example:\n\n```bash\ngit clone https://github.com/ibm-cloud-architecture/terraform-openshift3-vmware-example\n```\n\nThe files that you get from the GIT repo are:\n\n- `variables.tf`\n- `main.tf`\n- `infrastructure.tf`\n- `loadbalancer.tf`\n- `dns.tf`\n- `certs.tf`\n- `output.tf`\n\nIn the deployment, you must configure and review each of the `.tf` files for your infrastructure, and create and configure a `terraform.tfvars`.  We have attempted to separate them by the concerns in the filename.\n\nEach `*.tf` contains modules that can be invoked for the deployment that you may not needed depending on your configuration.  For example, if high-availability is not required, the `loadbalancer.tf` file can be deleted and related variables can be removed from the other files.\n\nOne thing that you should do is to decide on how you want to manage your DNS. Whether you are using CloudFlare with LetsEncrypt, an RFC2136 compliant Dynamic DNS, nip.io or others. If you choose others, then it is very advisable to perform the `/etc/hosts` file customization to ensure that all nodes are recognized properly.\n\n### Configuring Terraform\n\n#### Accessing vSphere\n\nOperation with the VMware infratructure is performed through the vSphere API. This section of the variables represents the resources that exists in the vSphere and must be identified to create the infrastructure. The snippet is below.\n\n```\n#######################################\n##### vSphere Access Credentials ######\n#######################################\nvsphere_server = \"vsphere-server.my-domain.com\"\n\n# Set username/password as environment variables VSPHERE_USER and VSPHERE_PASSWORD\n\n##############################################\n##### vSphere deployment specifications ######\n##############################################\n# Following resources must exist in vSphere\nvsphere_datacenter = \"CSPLAB\"\nvsphere_cluster = \"Sandbox\"\nvsphere_resource_pool = \"test-pool\"\ndatastore_cluster = \"SANDBOX_TIER4\"\n```\n\nIn the resources view, these are the vsphere hierarchy that must be identified to create the VM: \n\n![vsphere resource](assets/ocp/vsphere_resources.png)\n\nThe disk images for VMs are stored in either a Datastore or in a larger environments, a Datastore cluster. You can specify either of this, but not both, the example above is using the `datastore_cluster` the option is using the `datastore` option. Go to the datastore tab and the following is the illustration: \n  \n![datastore](assets/ocp/vsphere_datastore.png)\n\nNote that these names are case sensitive.\n\n#### vSphere storage class information\n\nThese values specify the vSphere username and password to access the datastore.  When Openshift is installed, a storageclass named `vsphere-standard` will be used to create block volumes on the datastore specified using the vSphere user and password.\n\n```\n# for the vsphere-standard storage class\nvsphere_storage_username = \"&lt;storageuser>\"\nvsphere_storage_password = \"&lt;storagepassword>\"\nvsphere_storage_datastore = \"ds01\"\n```\n\nPlease see the following [link](https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/vcp-roles.html) for the required permissions needed to by the storage user.\n\n\n#### Template \n\nThis information is needed also from the vSphere. OpenShift requires RedHat Enterprise Linux 7.4 or later VM template. You also should supply the credential to access the VMs that are created from the template. The credential can be using `ssh_user` and either one of `ssh_password` or `ssh_private_key_file`. The hostname prefix is used to prefix both the VM names and the hostname to be created in the DNS. Note that these names would have a random 8 hexadecimal characters to make sure that the hosts are unique.\n\n```\ntemplate = \"rhel-7.6-template\"\n# SSH username and private key to connect to VM template, has passwordless sudo access\nssh_user = \"virtuser\"\nssh_password = \"&lt;mypassword>\"\nssh_private_key_file = \"~/.ssh/id_rsa\"\n\n# MUST consist of only lower case alphanumeric characters and '-'\nhostname_prefix = \"ocp311\"\n```\n\nPlease see the following [link](ocp/RHEL_template) for information about template preparation.\n\n#### vSphere folder \n\nThe folder defined should not exist as the installation will create them, this folder may be a path on which the last qualifier will be created. \n\n```\n# vSphere Folder to provision the new VMs in, will be created\nfolder = \"openshift311-folder\"\n```\n\n#### Redhat account information\n\nThese are RedHat account to get either the RedHat Network subscription and getting the images for OpenShift. You can use the same username and password, but it is recommended that you create a RedHat service account for the image registry (see [these instructions](https://access.redhat.com/documentation/en-us/openshift_container_platform/3.11/html/configuring_clusters/install-config-configuring-red-hat-registry)). The RedHat subscription pool id can be retrieved from [this page](https://access.redhat.com/management/subscriptions?type=active) and select the subscription ID that you wanted to use.\n\n```\n# it's best to use a service account for these\nimage_registry_username = \"&lt;registry.redhat.io service account username>\"\nimage_registry_password = \"&lt;registry.redhat.io service acocunt password>\"\n\nrhn_username = \"&lt;rhn username>\"\nrhn_password = \"&lt;rhn password>\"\nrhn_poolid = \"&lt;rhn pool id>\"\n```\n\n#### Networking settings\n\nNetworking variables must be configured for the VMs. You may provide values for configuring both a private and public networks. see also ![network](assets/ocp/network.png). \n\nThe public network parameters are optional; if specified, it will place the bastion node on the public network.  In these scenarios you may want to stand up two load balancers on both the private and public networks to expose client traffic.\n\nAs a note, the example private network above would generate the first IP of `192.168.101.11` as the mask + offset + 1 gives you that address. You would need 4 addresses in the public network and the number of nodes + 1 for the private network. If you happen to have only a single flat network (ie the public and private network are the same network) then you would have to code the offset at least with a difference of 4.  \n\n```\n##### Network #####\nprivate_network_label = \"private_network\"\nprivate_staticipblock = \"192.168.101.0/24\"\nprivate_staticipblock_offset = 10           # IP assignment starts at 192.168.101.11\nprivate_netmask = \"24\"\nprivate_gateway = \"192.168.101.1\"\nprivate_domain = \"internal-network.local\"\nprivate_dns_servers = [ \"192.168.101.2\" ]\n```\n\nOptionally, if you want to place the cluster on multiple networks, the cluster nodes where the pod overlay network is set up can be a completely private network, while the bastion host and load balancers can be placed on an external network.\n\n```\npublic_network_label = \"external_network\"\npublic_staticipblock = \"10.30.65.0/24\"\npublic_staticipblock_offset = 30            # ips - [ 10.30.65.31, 10.30.65.32, 10.30.65.33 ]\npublic_netmask = \"24\"\npublic_gateway = \"10.30.65.1\"\npublic_domain = \"my-public-domain.com\"\npublic_dns_servers = [ \"1.1.1.1\" ]\n```\n\nThis Terraform template also supports non-contiguous ip addresses. You can specify specific node IP addresses. In this example, the terraform will provision two worker nodes, at `.13` and `.14` while there will be 3 storage nodes, at `.15`, `.16`, `.17`. \n\n```\n##### Network #####\nprivate_network_label = \"private_network\"\nbastion_private_ip = [\"192.168.0.10\"]\nmaster_private_ip = [\"192.168.0.11\"]\ninfra_private_ip = [\"192.168.0.12\"]\nworker_private_ip = [\"192.168.0.13\", \"192.168.0.14\"]\nstorage_private_ip = [\"192.168.0.15\", \"192.168.0.16\", \"192.168.0.17\"]\n\nprivate_netmask = \"24\"\nprivate_gateway = \"192.168.0.1\"\nprivate_domain = \"my-private-domain.local\"\nprivate_dns_servers = [ \"192.168.0.1\" ]\n```\n\n#### DNS settings\n\nThe `master_cname` and `app_cname` may be manually defined in the DNS for accessing the console and application routes, or added automatically using one of the DNS modules.  The `master_cname` is a CNAME record in DNS pointing at the master node or a load balancer distributing traffic to the master nodes.  The `app_cname` is a wildcard CNAME record in DNS pointing at the infra node or a load balancer distributing traffic to the infra nodes.\n\n```\n# these were added to my public DNS, the app_cname is a wildcard\nmaster_cname = \"ocp-console.my-public-domain.com\"\napp_cname = \"ocp-apps.my-public-domain.com\"\n```\n\n#### Nodes definition and sizing\n\nthis section defines the number of nodes for each kinds and how large is the vcpu, memory and disk sizes are. The disk size minimal is determined by the template, it must be the same or larger than the template that you start on. \n\n```\n# node definitions\nmaster = {\n  nodes = \"3\"\n  vcpu = \"8\"\n  memory = \"16384\"\n\n  disk_size = \"100\"\n  docker_disk_size = \"100\"\n  thin_provisioned = \"true\"\n  keep_disk_on_remove = false\n  eagerly_scrub = false\n}\n\ninfra = {\n  nodes = \"3\"\n  vcpu = \"8\"\n  memory = \"32768\"\n\n  disk_size = \"100\"\n  docker_disk_size = \"100\"\n  thin_provisioned = \"true\"\n  keep_disk_on_remove = false\n  eagerly_scrub = false\n}\n\nworker = {\n  nodes = \"3\"\n  vcpu = \"8\"\n  memory = \"32768\"\n\n  disk_size = \"100\"\n  docker_disk_size = \"100\"\n  thin_provisioned = \"true\"\n  keep_disk_on_remove = false\n  eagerly_scrub = false\n}\n\nstorage = {\n  nodes = \"3\"\n  vcpu = \"4\"\n  memory = \"16384\"\n\n  disk_size = \"100\"\n  docker_disk_size = \"100\"\n  gluster_num_disks = \"1\"\n  gluster_disk_size = \"200\"\n  thin_provisioned = \"true\"\n  keep_disk_on_remove = false\n  eagerly_scrub = false\n}\n```\n\n#### vSphere Credentials\n\nOnce you have all the variables customized, you also need to setup some environment variables to store your credentials:\n\n- `VSPHERE_USER` and `VSPHERE_PASSWORD`: user ID and password to access the VMware vSphere environment\n\nUse the following command:\n\n```bash\nexport VSPHERE_USER=&lt;user>\nexport VSPHERE_PASSWORD=&lt;password>\n```\n\n### Provision the environment using Terraform\n\n- Initialize Terraform environments (pulling in modules and plugins)\n\n  ```bash\n  terraform init\n  ```\n\n- use plan to see what terraform will do and validate the variables are correct:\n\n  ```bash\n  terraform plan\n  ```\n\n- Provision the environment\n\n  ```bash\n  terraform apply -auto-approve\n  ```\n\nThe result should gives you the OpenShift 3.11 environment.\n\nYou should be able to access the environment using the URL of `https://ocp-console.&lt;domain>` and logging in initially as `admin` with the password of `admin`.  Once additional set up of the environment is complete, you may want to configure a different identity provider for Openshift by following the [documentation](https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html).\n","fileAbsolutePath":"/Users/Shared/VBDData/programs/cloudpak8s/src/pages/_content/ocp/openshift3_terraform_installation_vsphere.md"}}}}